{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Contextual-Policy.ipynb","provenance":[],"authorship_tag":"ABX9TyOvziOQNE3YQlUXyIavziat"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FXH4qlBu7vsl","executionInfo":{"status":"ok","timestamp":1604504913404,"user_tz":-540,"elapsed":1597,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["import tensorflow as tf\n","import tensorflow.contrib.slim as slim\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TElKRhu8g_Y","executionInfo":{"status":"ok","timestamp":1604505018567,"user_tz":-540,"elapsed":1215,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["class contextual_bandit():\n","    def __init__(self):\n","        self.state = 0\n","        #List out our bandits. \n","        self.bandits = np.array([\n","             [0.2, 0, 0 ,-5],\n","             [0.1, -5, 1, 0.25],\n","             [-5, 5, 5, 5]\n","        ])\n","        self.num_bandits = self.bandits.shape[0]\n","        self.num_actions = self.bandits.shape[1]\n","        \n","    def getBandit(self):\n","        self.state = np.random.randint(0,len(self.bandits)) # get a random state for each episode.\n","        return self.state\n","        \n","    def pullArm(self,action):\n","        bandit = self.bandits[self.state,action]\n","        result = np.random.randn(1)\n","        if result > bandit:\n","            #return a positive reward.\n","            return 1\n","        else:\n","            #return a negative reward.\n","            return -1"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrwEdNC1855p","executionInfo":{"status":"ok","timestamp":1604505023782,"user_tz":-540,"elapsed":678,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["class agent():\n","    def __init__(self, lr, s_size, a_size):\n","        # These lines established the feed-forward part of the network. \n","        # The agent takes a state and produces an action.\n","        self.state_in= tf.placeholder(shape=[1],dtype=tf.int32)\n","        state_in_OH = slim.one_hot_encoding(self.state_in,s_size)\n","        \n","        output = slim.fully_connected(state_in_OH, a_size,           \n","            biases_initializer=None, activation_fn=tf.nn.sigmoid, weights_initializer=tf.ones_initializer())\n","        '''\n","        `fully_connected` creates a variable called `weights`,\n","        representing a fully connected weight matrix, which is multiplied by the `inputs`\n","        '''      \n","        self.output = tf.reshape(output,[-1])\n","        self.chosen_action = tf.argmax(self.output, 0)\n","\n","        # The next six lines establish the training proceedure.\n","        # We feed the reward and chosen action into the network\n","        # to compute the loss, and use it to update the network.\n","        self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n","        self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n","        self.responsible_weight = tf.slice(self.output,self.action_holder,[1])\n","        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n","        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n","        self.update = optimizer.minimize(self.loss)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvPF5Lha88jY","executionInfo":{"status":"ok","timestamp":1604505045024,"user_tz":-540,"elapsed":720,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}},"outputId":"62a06d3b-572d-46a2-db57-48638adf606b","colab":{"base_uri":"https://localhost:8080/"}},"source":["tf.reset_default_graph() # Clear the Tensorflow graph.\n"," \n","cBandit = contextual_bandit() # Load the bandits.\n","myAgent = agent(lr=0.001, s_size=cBandit.num_bandits, a_size=cBandit.num_actions) # Load the agent.\n","weights = tf.trainable_variables()[0]  # The weights we will evaluate to look into the network.\n","\n","total_episodes = 10000 # Set total number of episodes to train agent on.\n","total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions]) # Set scoreboard for bandits to 0.\n","e = 0.1 # Set the chance of taking a random action."],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:2563: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fca4ae53828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fca4ae53828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fca4ae53828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fca4ae53828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dwtB5oUb8_Em","executionInfo":{"status":"ok","timestamp":1604505082918,"user_tz":-540,"elapsed":8267,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}},"outputId":"59fd61f8-2acd-4c86-ea2e-a7c882658de7","colab":{"base_uri":"https://localhost:8080/"}},"source":["init = tf.global_variables_initializer()\n","\n","# Launch the tensorflow graph\n","with tf.Session() as sess:\n","    sess.run(init)\n","    i = 0\n","    while i < total_episodes:\n","        s = cBandit.getBandit() # Get a state from the environment.\n","        \n","        # Choose either a random action or one from our network.\n","        if np.random.rand(1) < e:\n","            action = np.random.randint(cBandit.num_actions)\n","        else:\n","            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in:[s]})\n","        \n","        reward = cBandit.pullArm(action) # Get our reward for taking an action given a bandit.\n","        \n","        # Update the network.\n","        feed_dict={myAgent.reward_holder:[reward], \n","                   myAgent.action_holder:[action],\n","                   myAgent.state_in:[s]}\n","        \n","        _, ww = sess.run([myAgent.update,weights], feed_dict=feed_dict)\n","        \n","        # Update our running tally of scores.\n","        total_reward[s,action] += reward\n","        if i % 500 == 0:\n","            print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward, axis=1)))\n","        i+=1\n","\n","for a in range(cBandit.num_bandits):\n","    print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n","    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n","        print(\"...and it was right!\")\n","    else:\n","        print(\"...and it was wrong!\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mean reward for each of the 3 bandits: [757.25 740.25 720.75]\n","Mean reward for each of the 3 bandits: [795.25 771.5  757.5 ]\n","Mean reward for each of the 3 bandits: [838.25 810.25 790.75]\n","Mean reward for each of the 3 bandits: [873.   847.25 833.  ]\n","Mean reward for each of the 3 bandits: [906.5  885.   871.75]\n","Mean reward for each of the 3 bandits: [945.75 923.5  910.  ]\n","Mean reward for each of the 3 bandits: [980.5  964.   947.25]\n","Mean reward for each of the 3 bandits: [1022.5  1000.5   983.25]\n","Mean reward for each of the 3 bandits: [1061.   1035.5  1022.25]\n","Mean reward for each of the 3 bandits: [1098.25 1072.5  1061.5 ]\n","Mean reward for each of the 3 bandits: [1131.   1111.75 1101.  ]\n","Mean reward for each of the 3 bandits: [1169.   1144.75 1137.5 ]\n","Mean reward for each of the 3 bandits: [1212.   1178.75 1172.5 ]\n","Mean reward for each of the 3 bandits: [1249.75 1214.75 1209.25]\n","Mean reward for each of the 3 bandits: [1288.   1251.75 1248.  ]\n","Mean reward for each of the 3 bandits: [1324.75 1289.   1288.5 ]\n","Mean reward for each of the 3 bandits: [1361.5  1328.   1325.75]\n","Mean reward for each of the 3 bandits: [1397.25 1367.5  1361.  ]\n","Mean reward for each of the 3 bandits: [1433.75 1408.   1396.  ]\n","Mean reward for each of the 3 bandits: [1468.25 1450.25 1427.25]\n","The agent thinks action 4 for bandit 1 is the most promising....\n","...and it was right!\n","The agent thinks action 2 for bandit 2 is the most promising....\n","...and it was right!\n","The agent thinks action 1 for bandit 3 is the most promising....\n","...and it was right!\n"],"name":"stdout"}]}]}