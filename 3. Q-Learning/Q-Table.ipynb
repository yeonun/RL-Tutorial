{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q-Table.ipynb","provenance":[],"authorship_tag":"ABX9TyNEuS3HsdAkFN1Zo9/4TPmK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ESMgwKx24mbp","executionInfo":{"status":"ok","timestamp":1604587781641,"user_tz":-540,"elapsed":1108,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["import gym\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"nK1iE5o04oYM","executionInfo":{"status":"ok","timestamp":1604587789687,"user_tz":-540,"elapsed":897,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["env = gym.make('FrozenLake-v0')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"68CWkMJF4rW7","executionInfo":{"status":"ok","timestamp":1604587808360,"user_tz":-540,"elapsed":763,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}},"outputId":"0193ed6c-8260-42d0-99f2-6b5bd233f46f","colab":{"base_uri":"https://localhost:8080/"}},"source":["Q = np.zeros([env.observation_space.n, env.action_space.n])\n","observation = env.reset()\n","env.render()\n","print('------------------------')\n","for i in range(env.action_space.n):\n","    action = i\n","    observation, reward, done, info = env.step(action)\n","    env.render()\n","    print('action: %d' %action)\n","    print('state : %d \\ninfo : %.3f' %(observation,info['prob']))\n","    print('------------------------')\n","    observation = env.reset()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","------------------------\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","action: 0\n","state : 4 \n","info : 0.333\n","------------------------\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","action: 1\n","state : 0 \n","info : 0.333\n","------------------------\n","  (Right)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","action: 2\n","state : 4 \n","info : 0.333\n","------------------------\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","action: 3\n","state : 0 \n","info : 0.333\n","------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_0cP2TtY4tw4","executionInfo":{"status":"ok","timestamp":1604587825515,"user_tz":-540,"elapsed":646,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}}},"source":["# Set learning parameters\n","lr = .8   # learning rate\n","y = .95   # discount factor\n","num_episodes = 2000\n","\n","#create lists to contain total rewards and steps per episode\n","rList = [] # reword list\n","# sList = [] # state list"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ES93L8e340V9","executionInfo":{"status":"ok","timestamp":1604587859728,"user_tz":-540,"elapsed":2822,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}},"outputId":"c1769fa7-883c-4f26-fcc4-0157d897ed46","colab":{"base_uri":"https://localhost:8080/"}},"source":["for i in range(num_episodes):\n","    s = env.reset() # Reset environment and get first new observation\n","    rAll = 0        # total reward\n","    d = False       # end of precess\n","    j = 0           # step\n","#     print('------<initial state>-------')\n","#     env.render()\n","#     print('state : %d \\ninfo : %.3f' %(s,info['prob']))\n","#     print('----------------------------')\n","    #The Q-Table learning algorithm\n","    while j < 100:\n","        j+=1\n","        \n","        # Choose an action by greedily (with noise) picking from Q table\n","        # 1/ (i+1) factor has the effect of cutting back on randomness\n","        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))  \n","        \n","        #Get new state and reward of action an agent did from environment\n","        s1, r, d, info = env.step(a)\n","        \n","        # Update Q-Table with new knowledge(=reward)\n","        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a]) \n","        \n","        rAll += r # add reward \n","        s = s1    # move to next state\n","\n","        # Check some conditions in console\n","#         env.render()\n","#         print('step : ')\n","#         print('action: %d' %a)\n","#         print('state : %d \\ninfo : %.3f' %(s,info['prob']))\n","#         print('----------------------------')\n","        \n","        # check the end of process\n","        if d == True:\n","#             if rAll == 1:\n","#                 print('Arrive at goal State!\\n')\n","\n","#             else:\n","#                 print('Arrive at hole. T.T\\n')\n","            break\n","    #jList.append(j)\n","    rList.append(rAll)\n","\n","print (\"Score over time: \" +  str(sum(rList)/num_episodes))\n","\n","print (\"Final Q-Table Values\")\n","print (np.round(Q,3))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Score over time: 0.566\n","Final Q-Table Values\n","[[0.4   0.007 0.012 0.002]\n"," [0.002 0.001 0.    0.009]\n"," [0.001 0.    0.021 0.001]\n"," [0.    0.    0.    0.017]\n"," [0.217 0.001 0.001 0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.001 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.009 0.004 0.    0.102]\n"," [0.    0.242 0.    0.002]\n"," [0.007 0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.    0.    0.    0.   ]\n"," [0.002 0.001 0.103 0.001]\n"," [0.    0.    0.811 0.   ]\n"," [0.    0.    0.    0.   ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LHJc-1w548J_","executionInfo":{"status":"ok","timestamp":1604587870851,"user_tz":-540,"elapsed":620,"user":{"displayName":"강영은","photoUrl":"","userId":"03412795950189262342"}},"outputId":"343f07d7-e0d7-40b6-ecb1-f1e135b3b4e0","colab":{"base_uri":"https://localhost:8080/"}},"source":["action_at_state=[]\n","action_set=['Left', 'Down', 'Right', 'Up']\n","for i in range(len(Q)):\n","    if np.sum(Q[i]) == 0:\n","        action_at_state.append('hole or goal state')\n","    else:\n","        idx=np.argmax(Q[i])\n","        action_at_state.append(action_set[idx])\n","print (\"at state, the agent move : \")\n","print (action_at_state)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["at state, the agent move : \n","['Left', 'Up', 'Right', 'Up', 'Left', 'hole or goal state', 'Left', 'hole or goal state', 'Up', 'Down', 'Left', 'hole or goal state', 'hole or goal state', 'Right', 'Right', 'hole or goal state']\n"],"name":"stdout"}]}]}